{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNFlBuiNON3akuTdIEB3QiI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3pnUW5Uo9RjR"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["1.\n","Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X) using a linear equation. The equation is typically represented as:\n","\n",">𝑌\n","=\n","𝛽\n","0\n","+\n","𝛽\n","1\n","𝑋\n","+\n","𝜖\n","where:\n","\n",">𝑌\n"," is the dependent variable (the outcome we are predicting),\n","\n",">𝑋\n"," is the independent variable (the predictor),\n","\n",">𝛽\n","0\n"," is the intercept (the value of\n",">𝑌\n"," when\n",">𝑋\n"," is zero),\n","\n",">𝛽\n","1\n"," is the slope (the rate of change of\n","𝑌\n"," with respect to\n","𝑋\n","),\n","\n",">𝜖\n"," represents the error term (the difference between the predicted and actual values)."],"metadata":{"id":"L9p-T0N_9VUt"}},{"cell_type":"markdown","source":["2. What are the key assumptions of Simple Linear Regression\n"," >The surrounding page content does not provide a direct explanation of the key assumptions of Simple Linear Regression, but it does list the question as part of an assignment. Based on general knowledge, the key assumptions of Simple Linear Regression are:\n","\n"," >   Linearity – The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n","\n"," >   Independence – The observations are independent of each other.\n","\n","  >   Homoscedasticity – The variance of residuals (errors) is constant across all levels of the independent variable.\n","\n"," >   Normality of Residuals – The residuals (differences between observed and predicted values) should be normally distributed.\n","\n"," >  No Multicollinearity – Since Simple Linear Regression involves only one independent variable, multicollinearity is not a concern (this applies more to Multiple Linear Regression)."],"metadata":{"id":"Qt5zQy-i91LZ"}},{"cell_type":"markdown","source":["3.  What does the coefficient m represent in the equation Y=mX+c\n","> The coefficient 𝑚\n"," in the equation\n","𝑌\n","=\n","𝑚𝑋+𝑐\n"," represents the slope of the line in a Simple Linear Regression model. It indicates the rate of change of the dependent variable\n","𝑌\n"," with respect to the independent variable\n","𝑋\n",".\n","\n"," > Mathematically,\n","𝑚\n"," is defined as:\n","\n"," > 𝑚\n","=\n","Δ\n","𝑌\n","Δ\n","𝑋\n","where:\n","\n"," > Δ\n","𝑌\n"," is the change in\n","𝑌\n",",\n","\n"," > Δ\n","𝑋\n"," is the change in\n","𝑋\n",".\n","\n"," > Interpretation:\n"," > If\n","𝑚\n"," is positive,\n","𝑌\n"," increases as\n","𝑋\n"," increases.\n","\n"," > If\n","𝑚\n"," is negative,\n","𝑌\n"," decreases as\n","𝑋\n"," increases.\n","\n"," > If\n","𝑚\n"," is zero,\n","𝑌\n"," remains constant regardless of\n","𝑋\n","."],"metadata":{"id":"DVh1UDR0-MtC"}},{"cell_type":"markdown","source":["4.  What does the intercept c represent in the equation Y=mX+c\n","> The intercept\n","𝑐\n"," in the equation\n","𝑌\n","=\n","𝑚\n","𝑋+𝑐\n"," represents the value of\n","𝑌\n"," when\n","𝑋\n"," is zero. It is the point where the regression line crosses the Y-axis.\n","\n"," > Interpretation:\n"," > If\n","𝑐\n"," is positive, the line starts above the origin.\n","\n"," > If\n","𝑐\n"," is negative, the line starts below the origin.\n","\n"," > If\n","𝑐\n"," is zero, the line passes through the origin."],"metadata":{"id":"-d_Wwkgj-q8e"}},{"cell_type":"markdown","source":["5.  How do we calculate the slope m in Simple Linear Regression\n","> The surrounding page content consists of a list of assignment questions related to regression, but it does not provide an explanation for how to calculate the slope\n","𝑚\n"," in Simple Linear Regression. Based on general knowledge, the slope\n","𝑚\n"," is calculated using the least squares method, which minimizes the sum of squared differences between observed and predicted values.\n","\n"," > The formula for\n","𝑚\n"," is:\n","\n"," > 𝑚\n","=\n","∑\n","(\n","𝑋\n","𝑖\n","−\n","𝑋\n","ˉ\n",")\n","(\n","𝑌\n","𝑖\n","−\n","𝑌\n","ˉ\n",")\n","∑\n","(\n","𝑋\n","𝑖\n","−\n","𝑋\n","ˉ\n",")\n","2\n","where:\n","\n"," > 𝑋\n","𝑖\n"," and\n","𝑌\n","𝑖\n"," are individual data points,\n","\n"," > 𝑋\n","ˉ\n"," and\n","𝑌\n","ˉ\n"," are the mean values of\n","𝑋\n"," and\n","𝑌\n",",\n","\n"," > The numerator represents the covariance between\n","𝑋\n"," and\n","𝑌\n",",\n","\n"," > The denominator represents the variance of\n","𝑋\n","."],"metadata":{"id":"lp4qeywO_A4-"}},{"cell_type":"markdown","source":["6.  What is the purpose of the least squares method in Simple Linear Regression\n","> The surrounding page content consists of a list of assignment questions related to regression, but it does not provide an explanation for the purpose of the least squares method in Simple Linear Regression. Based on general knowledge, the **least squares method** is used to find the best-fitting regression line by minimizing the sum of squared differences between observed and predicted values.\n","\n",">  Purpose of the Least Squares Method:\n","1. **Minimizes Error** – It ensures that the total squared difference between actual values (\\( Y_i \\)) and predicted values (\\( \\hat{Y_i} \\)) is as small as possible.\n","2. **Finds Optimal Slope and Intercept** – It calculates the best values for \\( m \\) (slope) and \\( c \\) (intercept) to make predictions as accurate as possible.\n","3. **Improves Model Accuracy** – By reducing the impact of outliers and variations in data, it provides a stable and reliable regression model.\n","4. **Ensures Unbiased Estimates** – It helps in obtaining unbiased estimates of the regression coefficients, making the model useful for predictions.\n","\n"],"metadata":{"id":"mMiRI8-g_Unz"}},{"cell_type":"markdown","source":["7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n","> The surrounding page content includes a list of assignment questions related to regression but does not provide an explanation for how the coefficient of determination (\\( R^2 \\)) is interpreted in Simple Linear Regression. Based on general knowledge:\n","\n","> ### Interpretation of \\( R^2 \\) in Simple Linear Regression:\n","The **coefficient of determination** (\\( R^2 \\)) measures how well the regression model explains the variability of the dependent variable (\\( Y \\)) based on the independent variable (\\( X \\)). It is calculated as:\n","\n"," > \\[\n","R^2 = 1 - \\frac{\\sum (Y_i - \\hat{Y_i})^2}{\\sum (Y_i - \\bar{Y})^2}\n","\\]\n","\n"," > where:\n","- \\( Y_i \\) are the actual values,\n","- \\( \\hat{Y_i} \\) are the predicted values from the regression model,\n","- \\( \\bar{Y} \\) is the mean of \\( Y \\).\n","\n"," > ### Key Interpretations:\n","- **\\( R^2 = 1 \\)** → The model perfectly predicts the dependent variable.\n","- **\\( R^2 = 0 \\)** → The model does not explain any variability in \\( Y \\).\n","- **Higher \\( R^2 \\) values** indicate a better fit, meaning the independent variable explains more of the variation in the dependent variable.\n","- **Lower \\( R^2 \\) values** suggest that other factors (not included in the model) influence \\( Y \\).\n","\n"],"metadata":{"id":"sSBo2JgK_t_Z"}},{"cell_type":"markdown","source":["\n","\n","### **8. What is Multiple Linear Regression?**\n","> Multiple Linear Regression (MLR) is an extension of Simple Linear Regression where the dependent variable (\\( Y \\)) is predicted using **two or more independent variables** (\\( X_1, X_2, ..., X_n \\)). The general equation is:\n","\n","> \\[\n","Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n","\\]\n","\n","> where:\n","- \\( Y \\) is the dependent variable (the outcome being predicted),\n","- \\( X_1, X_2, ..., X_n \\) are independent variables (predictors),\n","- \\( \\beta_0 \\) is the intercept,\n","- \\( \\beta_1, \\beta_2, ..., \\beta_n \\) are the coefficients representing the effect of each independent variable on \\( Y \\),\n","- \\( \\epsilon \\) is the error term.\n","\n"],"metadata":{"id":"EdJUylJQ__FY"}},{"cell_type":"markdown","source":["9. What is the main difference between Simple and Multiple Linear Regression\n","> The surrounding page content consists of a list of assignment questions related to regression but does not provide an explanation for the main difference between Simple and Multiple Linear Regression. Based on general knowledge:\n","\n",">### **Main Difference Between Simple and Multiple Linear Regression**\n","1. **Number of Independent Variables**:\n","   - **Simple Linear Regression** involves **one** independent variable (\\( X \\)) to predict the dependent variable (\\( Y \\)).\n","   - **Multiple Linear Regression** involves **two or more** independent variables (\\( X_1, X_2, ..., X_n \\)) to predict \\( Y \\).\n","\n",">2. **Equation Form**:\n","   - **Simple Linear Regression**:  \n","     \\[\n","     Y = \\beta_0 + \\beta_1 X + \\epsilon\n","     \\]\n","   - **Multiple Linear Regression**:  \n","     \\[\n","     Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n","     \\]\n","\n",">3. **Complexity**:\n","   - **Simple Linear Regression** is easier to interpret and visualize (a straight line in 2D space).\n","   - **Multiple Linear Regression** is more complex, requiring higher-dimensional analysis.\n","\n",">4. **Use Cases**:\n","   - **Simple Linear Regression** is used when a single factor influences \\( Y \\).\n","   - **Multiple Linear Regression** is used when multiple factors contribute to \\( Y \\), allowing for better predictions.\n","\n"],"metadata":{"id":"F0OiaWVgC-Xs"}},{"cell_type":"markdown","source":["10. What are the key assumptions of Multiple Linear Regression\n","> The surrounding page content consists of a list of assignment questions related to regression but does not provide an explanation for the key assumptions of Multiple Linear Regression. Based on general knowledge, the key assumptions of Multiple Linear Regression are:\n"," ### **Key Assumptions of Multiple Linear Regression**\n","> 1.**Linearity** – The relationship between the dependent variable (\\( Y \\)) and independent variables (\\( X_1, X_2, ..., X_n \\)) is linear.\n","\n"," > 2.**Independence** – Observations are independent of each other.\n","\n"," >  3.**Homoscedasticity** – The variance of residuals (errors) is constant across all levels of the independent variables.\n","\n"," >  4.**Normality of Residuals** – The residuals (differences between observed and predicted values) should be normally distributed.\n","\n"," >  5.**No Multicollinearity** – Independent variables should not be highly correlated with each other.\n","\n"," >  6.**No Autocorrelation** – Residuals should not be correlated with each other (important in time series data).\n","\n","\n"],"metadata":{"id":"xvV8zg19CJb3"}},{"cell_type":"markdown","source":["11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n","\n",">Heteroscedasticity occurs when the **variance of residuals (errors) is not constant** across all levels of the independent variables in a regression model. In other words, the spread of errors increases or decreases as the values of the independent variables change.\n","\n",">### **Effects of Heteroscedasticity on Multiple Linear Regression:**\n","1. **Biased Standard Errors** – It can lead to incorrect estimates of standard errors, affecting hypothesis testing.\n","2. **Inefficient Estimates** – The regression coefficients remain unbiased, but they may not be the most efficient (i.e., they have higher variance).\n","3. **Invalid Confidence Intervals** – Confidence intervals and significance tests (like t-tests) may be unreliable.\n","4. **Reduced Predictive Accuracy** – The model may not generalize well to new data.\n","\n"],"metadata":{"id":"t9ENh_oLC6vF"}},{"cell_type":"markdown","source":["12. How can you improve a Multiple Linear Regression model with high multicollinearity\n","\n","\n",">### **How to Improve a Multiple Linear Regression Model with High Multicollinearity**\n","Multicollinearity occurs when independent variables in a regression model are highly correlated, making it difficult to determine their individual effects on the dependent variable. To address this issue, consider the following techniques:\n","\n","1. **Remove Highly Correlated Predictors** – Identify and eliminate one or more variables that are strongly correlated using correlation matrices or Variance Inflation Factor (VIF).\n","2. **Use Principal Component Analysis (PCA)** – Transform correlated variables into a set of uncorrelated principal components.\n","3. **Apply Ridge Regression (L2 Regularization)** – Introduce a penalty term to reduce the impact of multicollinearity while keeping all predictors.\n","4. **Use Lasso Regression (L1 Regularization)** – Automatically selects important features by shrinking some coefficients to zero.\n","5. **Increase Sample Size** – A larger dataset can help reduce the effects of multicollinearity.\n","6. **Standardize or Normalize Variables** – Scaling features can sometimes mitigate multicollinearity.\n","7. **Use Domain Knowledge** – Select variables based on theoretical understanding rather than statistical correlation alone.\n","\n","Would you like a Python implementation to detect and handle multicollinearity?"],"metadata":{"id":"h1C1TVWICytr"}},{"cell_type":"markdown","source":["13.  What are some common techniques for transforming categorical variables for use in regression models\n","\n","\n",">### **Common Techniques for Transforming Categorical Variables in Regression Models**\n","1. **One-Hot Encoding** – Converts categorical variables into binary columns (0 or 1) for each category.\n","   - Example: *Color (Red, Blue, Green)* → *Red (1/0), Blue (1/0), Green (1/0)*\n","\n",">2. **Label Encoding** – Assigns numerical values to categories in a sequential manner.\n","   - Example: *Low = 0, Medium = 1, High = 2*\n","\n",">3. **Ordinal Encoding** – Used when categories have a meaningful order.\n","   - Example: *Education (High School = 1, Bachelor = 2, Master = 3, PhD = 4)*\n","\n",">4. **Binary Encoding** – Converts categories into binary numbers and represents them in separate columns.\n","   - Example: *Category A = 01, Category B = 10*\n","\n",">5. **Target Encoding** – Replaces categories with the mean of the target variable for each category.\n","   - Example: *Category A → Mean of Y for A*\n","\n",">6. **Frequency Encoding** – Assigns values based on the frequency of each category in the dataset.\n","   - Example: *Category A appears 50 times → Encoded as 50*\n","\n",">7. **Embedding Techniques** – Used in deep learning models to represent categorical variables as dense vectors.\n","\n"],"metadata":{"id":"nwcZOgc4ARwu"}},{"cell_type":"markdown","source":["14.  What is the role of interaction terms in Multiple Linear Regression\n",">The surrounding page content consists of a list of assignment questions related to regression but does not provide an explanation for the role of interaction terms in Multiple Linear Regression. Based on general knowledge:\n","\n"," > ### **Role of Interaction Terms in Multiple Linear Regression**\n","Interaction terms in Multiple Linear Regression capture the **combined effect** of two or more independent variables on the dependent variable. They help model situations where the effect of one predictor depends on the value of another predictor.\n","\n"," > ### **How Interaction Terms Work:**\n"," > - The standard Multiple Linear Regression model is:\n","  \\[\n","  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n","  \\]\n"," > - When an interaction term is included, the model becomes:\n","  \\[\n","  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon\n","  \\]\n"," > - The term \\( \\beta_3 (X_1 \\times X_2) \\) represents the interaction effect.\n","\n","\n"],"metadata":{"id":"scsbIpoBEBgN"}},{"cell_type":"markdown","source":["15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",">\n","### **Interpretation of Intercept in Regression Models**\n","The **intercept** (\\( c \\) or \\( \\beta_0 \\)) in a regression equation represents the predicted value of the dependent variable (\\( Y \\)) when all independent variables (\\( X \\)) are **zero**.\n","\n",">### **Difference Between Simple and Multiple Linear Regression:**\n","1. **Simple Linear Regression**:\n","   - Equation:  \n","     \\[\n","     Y = \\beta_0 + \\beta_1 X + \\epsilon\n","     \\]\n","   - **Interpretation**: The intercept (\\( \\beta_0 \\)) represents the expected value of \\( Y \\) when \\( X = 0 \\).\n","   - **Example**: If predicting salary based on years of experience, \\( \\beta_0 \\) represents the estimated salary for someone with **zero experience**.\n","\n",">2. **Multiple Linear Regression**:\n","   - Equation:  \n","     \\[\n","     Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n","     \\]\n","   - **Interpretation**: The intercept (\\( \\beta_0 \\)) represents the expected value of \\( Y \\) when **all independent variables are zero**.\n","   - **Example**: If predicting house price based on square footage and number of bedrooms, \\( \\beta_0 \\) represents the estimated price of a house with **zero square footage and zero bedrooms**, which may not be meaningful.\n","\n","  > ### **Key Differences:**\n"," >- In **Simple Linear Regression**, the intercept often has a more intuitive interpretation.\n"," >- In **Multiple Linear Regression**, the intercept may not always be meaningful, especially if zero values for all predictors are unrealistic.\n"],"metadata":{"id":"ZMtuMI0aEYU_"}},{"cell_type":"markdown","source":["16.  What is the significance of the slope in regression analysis, and how does it affect predictions\n",">The slope in regression analysis represents the rate of change of the dependent variable (\n","𝑌\n",") with respect to the independent variable (\n","𝑋\n",").\n","\n"," >A positive slope means\n","𝑌\n"," increases as\n","𝑋\n"," increases.\n","\n"," >A negative slope means\n","𝑌\n"," decreases as\n","𝑋\n"," increases.\n","\n"," >A zero slope means\n","𝑌\n"," remains constant regardless of\n","𝑋\n",". The slope affects predictions by determining how changes in\n","𝑋\n"," influence\n","𝑌\n","."],"metadata":{"id":"YqMoW1OXEwCp"}},{"cell_type":"markdown","source":["17. How does the intercept in a regression model provide context for the relationship between variables\n",">𝑅\n","2\n"," only measures how well the model explains variance but does not indicate predictive accuracy.\n","\n"," >It does not account for overfitting or whether the model is appropriate for the data.\n","\n"," >A high\n","𝑅\n","2\n"," does not always mean a good model, especially if irrelevant variables are included."],"metadata":{"id":"CmIp38_eAPk7"}},{"cell_type":"markdown","source":["18. How would you interpret a large standard error for a regression coefficient\n","> A large standard error suggests that the coefficient estimate is unstable and may vary significantly across samples.\n","\n"," >It indicates low precision, meaning the predictor may not have a strong effect on\n","𝑌\n",".\n","\n"," >Possible causes include\n","multicollinearity or insufficient data."],"metadata":{"id":"41ctiWgZFbHi"}},{"cell_type":"markdown","source":["19.  How would you interpret a large standard error for a regression coefficient\n",">A large standard error suggests that the coefficient estimate is unstable and may vary significantly across samples.\n","\n"," >It indicates low precision, meaning the predictor may not have a strong effect on\n","𝑌\n",".\n","\n"," >Possible causes include multicollinearity or insufficient data."],"metadata":{"id":"TKN93uVCFi-U"}},{"cell_type":"markdown","source":["20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it\n"," >Heteroscedasticity occurs when the variance of residuals is not constant across different values of the independent variable. It can be identified using:\n","\n"," >Residual vs. Fitted Value Plot – If residuals spread out unevenly (forming a cone shape), heteroscedasticity is present.\n","\n"," >Breusch-Pagan Test – A statistical test to detect heteroscedasticity.\n","\n"," >White Test – Another formal test for heteroscedasticity.\n","\n"," > Scatter Plot of Residuals – If residuals show increasing or decreasing spread, it indicates heteroscedasticity."],"metadata":{"id":"8soQHljyGMLR"}},{"cell_type":"markdown","source":["21.  What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",">A high\n","𝑅\n","2\n"," but low adjusted\n","𝑅\n","2\n"," in a Multiple Linear Regression model suggests that additional predictors are not significantly improving the model's performance. Here’s why:\n","\n"," >Interpretation:\n","High\n","𝑅\n","2\n"," – Indicates that the model explains a large portion of the variance in the dependent variable.\n","\n"," >Low Adjusted\n","𝑅\n","2\n"," – Suggests that some independent variables may be irrelevant or redundant, meaning they do not contribute meaningful predictive power."],"metadata":{"id":"uN8A3DHJGfL8"}},{"cell_type":"markdown","source":["22.  Why is it important to scale variables in Multiple Linear Regression\n",">Prevents Features with Large Magnitudes from Dominating\n","\n"," >If one variable has a much larger scale (e.g., population size in millions vs. age in years), the regression coefficients may be disproportionately affected.\n","\n"," >Improves Model Interpretability\n","\n"," >Standardizing variables (subtracting the mean and dividing by the standard deviation) makes regression coefficients more comparable.\n","\n"," >Enhances Performance in Regularized Models\n","\n"," >Techniques like Ridge Regression and Lasso Regression apply penalties to coefficients, and scaling ensures fair treatment of all variables.\n","\n"," >Speeds Up Convergence in Gradient-Based Optimization\n","\n"," >Normalizing data helps models using gradient descent (e.g., Lasso, Ridge) converge faster and avoid getting stuck in local minima"],"metadata":{"id":"yIWto4KmGtF6"}},{"cell_type":"markdown","source":["23.  What is polynomial regression\n",">Polynomial regression extends linear regression by including higher-degree terms (\n","𝑋\n","2\n",",\n","𝑋\n","3\n",",\n"," etc.) to model nonlinear relationships."],"metadata":{"id":"HLq6FrDRG_qA"}},{"cell_type":"markdown","source":["24.  How does polynomial regression differ from linear regression\n",">The intercept represents the predicted value of\n","𝑌\n"," when all independent variables are zero.\n","\n"," >In Simple Linear Regression, it often has a meaningful interpretation.\n","\n"," >In Multiple Linear Regression, it may not always be meaningful, especially if zero values for all predictors are unrealistic."],"metadata":{"id":"FrTKRibKHLHE"}},{"cell_type":"markdown","source":["25.  When is polynomial regression used\n",">When the relationship between\n","𝑋\n"," and\n","𝑌\n"," is nonlinear and cannot be captured by a straight line.\n","\n"," >Used in cases where data shows curvature."],"metadata":{"id":"2QJBcn0KHW40"}},{"cell_type":"markdown","source":["26.  What is the general equation for polynomial regression\n",">𝑌\n","=\n","𝛽\n","0+\n","𝛽\n","1\n","𝑋+\n","𝛽\n","2𝑋2+...+\n","𝛽\n","𝑛\n","𝑋\n","𝑛+\n","𝜖"],"metadata":{"id":"vZLZpcSqHdVM"}},{"cell_type":"markdown","source":["27.  Can polynomial regression be applied to multiple variables\n",">Yes, polynomial terms can be added for multiple independent variables."],"metadata":{"id":"tKJo0AseHzf6"}},{"cell_type":"markdown","source":["28.  What are the limitations of polynomial regression\n",">Susceptible to overfitting if the degree is too high.\n","\n"," >Requires careful selection of polynomial degree.\n","\n"," >Computationally expensive for high-degree polynomials."],"metadata":{"id":"8D4ekqLcH5dq"}},{"cell_type":"markdown","source":["29.  What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",">Cross-validation\n","\n"," >Adjusted\n","𝑅\n","2\n","\n"," >AIC/BIC (Akaike/Bayesian Information Criterion)\n","\n"," >Residual analysis"],"metadata":{"id":"BokBXyd9IEsy"}},{"cell_type":"markdown","source":["30.  Why is visualization important in polynomial regression\n",">Helps assess the appropriateness of polynomial terms.\n","\n"," >Detects overfitting or underfitting."],"metadata":{"id":"WVRYYZI4INon"}},{"cell_type":"markdown","source":["31.  How is polynomial regression implemented in Python?\n",">Use PolynomialFeatures from sklearn.preprocessing to transform features.\n","\n"," >Apply linear regression after transformation."],"metadata":{"id":"tdhkWtsVIVbF"}}]}